{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모형결합 (model combining)\n",
    "## 앙상블 방법론 (ensemble methods)\n",
    "* 예측 성능을 향상시키기 위해서 하나의 모형이 아닌 여러개의 모형을 결합하는 방법\n",
    "* 장점 : 과적합 방지 등을 통해 성능 향상\n",
    "* 단점 : 단일 모형을 사용하는 것 보다 계산량 증가\n",
    "\n",
    "# 앙상블 방법론 - 취합(aggregation) & 부스팅(boosting)\n",
    "## 취합(aggregation)\n",
    "* 사용할 모형의 집합을 처음부터 **고정**함\n",
    "* Ex : 다수결(Majority Voting), 배깅(Bagging), 랜덤포레스트(Random Forest)\n",
    "    -  다수결 방법(Majority Voting)\n",
    "        * 여러 종류의 모형들 중 가장 좋은 결과를 가진 모델 채택\n",
    "        * 채택 방법 :\n",
    "            * Hard Voting\n",
    "                * 단순 투표, 가장 많이 나온 결과를 채택 (Dafault)\n",
    "            * Soft Voting\n",
    "                * 가중치 투표, 개별 모형의 조건부 확률들을 합한 것들 중 가장 큰 것을 재택함\n",
    "    - 배깅(Bagging)\n",
    "        * 같은 모형을 사용하지만 같은 데이터 샘플을 중복으로 사용하여, 서로 다른 결과를 출력하는 다수의 모형을 사용하는 방법\n",
    "        * BaggingClassifier\n",
    "            - base_estimator : 기본 모형\n",
    "            - n_estimators : 모형 개수(default = 10)\n",
    "            - bootstrap : 데이터 중복 사용 여부(default = True)\n",
    "            - max_samples : 데이터 샘플 중 선택할 샘플의 수 또는 비율(default = 1.0)\n",
    "            - bootstrap_features : 특징 차원의 중복 사용 여부(default = False)\n",
    "            - max_features : 다차원 독립 변수 중 선택할 차원의 수 혹은 비율 (default = 1.0)\n",
    "\n",
    "## 부스팅(Boosting)\n",
    "* 사용할 모형을 **점진적으로 늘려가는** 방법\n",
    "* Ex : 에이다부스트(AdaBoost), 그레디언트 부스트(GradientBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] 다수결 방법 - 신용카드 데이터셋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(max_iter=1000, random_state=1)\n",
      "학습용: 0.9529860228716646\n",
      "검증용: 0.9289340101522843\n",
      "\n",
      "DecisionTreeClassifier(random_state=1)\n",
      "학습용: 1.0\n",
      "검증용: 0.9289340101522843\n",
      "\n",
      "KNeighborsClassifier(n_neighbors=2)\n",
      "학습용: 0.9428208386277002\n",
      "검증용: 0.9289340101522843\n",
      "\n",
      "VotingClassifier(estimators=[('lr',\n",
      "                              LogisticRegression(max_iter=1000,\n",
      "                                                 random_state=1)),\n",
      "                             ('tree', DecisionTreeClassifier(random_state=1)),\n",
      "                             ('knn', KNeighborsClassifier(n_neighbors=2))],\n",
      "                 voting='soft')\n",
      "학습용: 1.0\n",
      "검증용: 0.9441624365482234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/creditcard.csv')\n",
    "\n",
    "# 독립변수 컬럼 리스트 : 처음컬럼빼고 다 가져오기\n",
    "train_cols = df.columns[1:-1]\n",
    "\n",
    "# 독립변수, 종속변수 설정\n",
    "X = df[train_cols]\n",
    "y = df[\"Class\"]\n",
    "\n",
    "#언더 샘플링\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_sample, y_sample = RandomUnderSampler(random_state=0).fit_resample(X, y)\n",
    "X_samp = pd.DataFrame(data=X_sample, columns=train_cols)\n",
    "y_samp = pd.DataFrame(data=y_sample, columns=['Class'])\n",
    "df2 = pd.concat([X_samp, y_samp], axis=1)\n",
    "\n",
    "#독립변수, 종속변수\n",
    "X = X_samp[train_cols]\n",
    "y = y_samp[\"Class\"]\n",
    "\n",
    "#학습&검증용 데이터\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "#여러 모델 선언\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model1 = LogisticRegression(random_state= 1, max_iter=1000)\n",
    "model2 = DecisionTreeClassifier(random_state= 1)\n",
    "model3 = KNeighborsClassifier(n_neighbors= 2)\n",
    "\n",
    "# estimators : 개별 모형 목록, 리스트나 named parameter 형식으로 입력\n",
    "# voting : 문자열 {hard, soft} hard voting 과 soft voting 선택. 디폴트 hard\n",
    "# 3개 모델 합친 모형\n",
    "ensemble = VotingClassifier(estimators=[('lr', model1), ('tree', model2), ('knn', model3)], voting='soft')\n",
    "\n",
    "for model in (model1, model2, model3, ensemble):\n",
    "    print(model)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"학습용:\", model.score(X_train, y_train))\n",
    "    print(\"검증용:\", model.score(X_test, y_test))\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] 모형결합 (배깅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/customer.csv\")\n",
    "\n",
    "\n",
    "df[\"Churn\"].value_counts()\n",
    "\n",
    "train_cols = df.columns[0:16]\n",
    "\n",
    "X = df[train_cols]\n",
    "y = df[\"Churn\"]\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "X_sample, y_sample = RandomUnderSampler(random_state=0).fit_resample(X, y)\n",
    "\n",
    "X_samp = pd.DataFrame(data=X_sample, columns=train_cols)\n",
    "y_samp = pd.DataFrame(data=y_sample, columns=['Churn'])\n",
    "df_samp = pd.concat([X_samp, y_samp], axis=1)\n",
    "\n",
    "df_samp[\"Churn\"].value_counts()\n",
    "\n",
    "X = df_samp[train_cols]\n",
    "y = df_samp[\"Churn\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model1 = DecisionTreeClassifier(random_state=0)\n",
    "model2 = BaggingClassifier(DecisionTreeClassifier(), n_estimators=100, random_state=0)\n",
    "\n",
    "# for model in (model1, model2):\n",
    "#     print(model)\n",
    "#     model.fit(X_train, y_train)\n",
    "#     print(\"학습용:\", model.score(X_train, y_train))\n",
    "#     print(\"검증용:\", model.score(X_test, y_test))\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [실습] 랜덤포레스트\n",
    "#### 랜덤포레스트\n",
    "- 의사결정나무를 개별 모형으로 사용하는 모형 결합 방법\n",
    "- 배깅의 일종으로, 배깅과 다르게 설명변수를 무작위로 선택함으로써 트리의 다양성을 확보해 모형간의 상관관계를 줄이고자 함\n",
    "- 배깅은 모형 종류에 제한이 없으나, **랜덤포레스트는 의사결정나무 모형만을 사용함**\n",
    "- 독립변수의 차원을 랜덤하게 감소시킨 후 독립변수를 선택하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n",
       "       'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/heart.csv')\n",
    "\n",
    "df[\"target\"].value_counts()\n",
    "\n",
    "df.columns"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc836340034460e9c5583b996b053fe1fe7ffeb52054821d3ff9d502e04de020"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('woodeem')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
